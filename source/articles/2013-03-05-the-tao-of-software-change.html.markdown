--- 
title: "The Tao of Software Change"
description: "One of the best parts of my job is that I get to work on really big systems. I love the scaling problems I encounter, and as a manager they don't go away. Let's talk about a process that's at the intersection of technology and human judgment: how to upgrade large pieces of software."
date: 2013-03-05 
popular: false
published: true
tags: management, process
---

Let me tell you about a bad day that you're going to have. It's going to start like any other day, you'll get up and take a shower, have a nice cup of coffee, maybe it's even better than an average day - the cute barista smiles as you get your change. At work your morning is pretty typical, you answer some email and approve a change request from your team to push out the new feature, which you note with a smile is going out early. You fly through your meetings and are ready to head home early, when you get a phone call. It's Sam, your senior systems engineer, he's calling to let you know that the new change you approved had some problems. The tests all passed, but when they deployed to production something was wrong and it caused an outage. 

You start asking for the details and discover that it caused a *big* outage - the service was completely down for almost half an hour before the team got a spike in customer contacts, and began to roll back the change. It took another 20 minutes before things returned to normal. The new feature isn't going to go out early like you thought. In fact, you're going to be spending the next week writing up a report on what happened to upper management, so it's probably going to go out late. I'm really sorry I had to be the one to tell you about this, but the reality is that change management is hard to do correctly and only getting harder. The odds were stacked against you, but still... things could have gone better. 

# The state of the art increases the opportunity for bad days
Software has thankfully moved away from the shrink wrapped distribution model, and into a continuously updating, always available on line model. Web services, cloud computing, mobile, and a whole bunch of other buzz words from the past decade have further increased the size and complexity of software, driving demand for horizontal growth - past a single computer and on to tens, hundreds, thousands... and beyond. 

Horizontal growth of applications has drastically increased the complexity and risk of updating software. Gone are the days where an individual "release engineer" can realistically deploy software in an ad-hoc fashion. Software deployments and the manual processes around them are now a leading source of outages ([http://www.cs.cmu.edu/~priya/PDL-CMU-05-109.pdf](http://www.cs.cmu.edu/~priya/PDL-CMU-05-109.pdf)) for any sufficiently complex system. As the manager of a team running production services, I end up thinking about this topic a lot. Nothing will kill a great product quicker than flaky availability, unless you're twitter (sorry I had to). 

But wait it gets worse. In addition to the growing risks involved in deploying software, the industry is trending towards more frequent deployments. Take a quick look at my "[the best parts of agile](/articles/2013/01/10/best-parts-of-agile.html "The Best Parts of Agile")" article, and it's easy to see that the only way to deliver small pieces to your customers quickly is to **deploy software frequently**. Even if you're not on the agile clue train, you're still updating more than you used to. The Internet doesn't release a new version of software [once a year](http://techreport.com/news/23414/rumor-microsoft-may-hasten-windows-release-cycle). If you're slow you release monthly. That alone is an order of magnitude faster than the days of boxed software. 

# The Tao of Software Change
You're on the hook to make sure that in spite of all the extra opportunity for failure, it doesn't happen. But how? Let's start with the deployment mindset. Before you go and make sweeping changes, you've got to get on common ground with the rest of the dev team. The path to less bad days is different from the one you're currently on, and so everyone must adopt a new way. If you can develop the right belief system on your team around deploying software, most people will end up doing the right thing as a consequence of those beliefs. Without a Tao, you'll end up with well intentioned, but ultimately ineffective solutions to important problems. I'm dedicating this article solely to developing your Tao, it's that important. 

And now, let's TAOOOOOOO:

## Your problem isn't quality. Start with an assumption that all software is broken.
It's a typical reaction when you have a bad deployment to redouble your efforts on raising the quality bar of the team's software. Surely if we just get *a little better* at programming cautiously, or if we add just *a few more tests* this kind of thing won't happen. Wrong. It will. The deck is stacked against even the best, most defensive programmers. Each extra line of code is an opportunity for a bad interaction with what is surely a large and complicated system. Don't stop testing or trying to improve, but admit that you're never going to be able to account for all failure modes up front. Even if you have a good day and you nail it, can you do it again tomorrow? Next week? Every day for the next 3 years? You have to start with the assumption that what you're about to deploy has a bug in it.  

## Configuration is more dangerous than code.

Code, you're paying attention to that. You've got a test environment and that's helping but what about all the stuff that changes in production via configuration? Service endpoints, deployment settings, a different database, different caches, different application configuration, different... everything. Sure, it's more difficult to get this stuff wrong, because it's a couple lines of code sprinkled in a few well understood places (hah!). But when you DO get it wrong (you will. plan for failure), you've just caused a system-wide outage. The blast radius of a configuration change can be *huge*, so why are you waiting till production to find out you mistyped something, or that we changed the naming convention, or that the new endpoint isn't up because we deployed in the wrong order. That can't happen, so just start planning for how you verify configuration changes to production before it takes down the whole fleet. 

## The weakest link in your process is the people. 
Who's going to make a mistake first in a reproducible process, a computer or a person? Which one feels stress, gets tired, becomes distracted, or otherwise opens the door for an unintended outcome? It can be difficult as the manager to sell your team on the idea that they shouldn't be the ones making decisions in the heat of the moment. It comes across as a lack of faith in their abilities, or a gap in their performance which, once closed will make them well suited for this responsibility. Make them understand that this isn't a comment on their personal judgment. You need to get everyone to think less about improving the team's process, and more about replacing the process with a mechanism that doesn't involve human decision making. Extra auditing, additional scrutiny, or any other heavy weight process that slows things down all have diminishing returns. They'll get your release process to good, but they won't take it to great. You have to drive out the human element to get great.

## Be critical of powerful tools.
If a human can make a big mistake, a human with a powerful tool can make all the mistakes. Don't discourage automation, you need to do that to eliminate the weakest link in your process, but automation doesn't mean you build a tool that magnifies the power and impact of your weakest link. Audit your tools, make sure they have guard rails, and rate limiters. **Assume you will always type the wrong thing** how bad is that? Really bad? OK put the gun down, we've shot enough foots for one day. 

## Think of potential outages or exposed customers as mistake vectors.
Threat model your release process. No I'm being serious, go get everyone on your team, and ask them to assume that every part of the process is broken. Ask them how our monitors detects and alert us, ask them how far down the release process the bad change gets before we stop it. What was the blast radius? Did every customer experience an outage? Ask them how many opportunities we gave ourselves to find the problem before it's running on every production host in the fleet. What would the system do if a person wasn't present?

## You're **still** going to have a bad day. Reduce the blast radius.
Even if you're the best in the world at rapidly releasing software - mistakes, coordinated failures, unanticipated interactions, and other low-probability events will happen. Plan for the failure and think about how you limit its overall impact. This usually involves breaking things down into smaller groups, and being strategic about your exposure. It's unfortunate when you expose 1% of your customers to a bad software change. But it's a colossal fuck up when you expose 60%, 90%, 100%... don't turn a bad day into the worst day because you didn't think about how to limit scope of an impact.

# Move past test scenarios by adopting the Tao
This is the cultural mindset of a great operational team, because it involves admitting that failure is inevitable. Accept that you might be able to catch most of the bugs in testing, but not all of them. That you can get the configuration right most of the time, but not every time. A person will always do the right thing, until they forget. How the team collectively thinks about failure - from preventable to inevitable - moves them down the path. It breaks an irrational believe that you can test everything before production.

Many teams only ever get to the point where they're testing for the known problems. Once the integration tests are passing there's nothing more to be done, so off to production it goes. Test scenarios are necessary, but not sufficient for a low impact change management process. The Tao of Software Change is intended to highlight an underlying set of problems which basic testing alone can never prevent. Once the team has internalized this as a reality, everyone can come together and start building a better system that's focused on issue containment. 

Let us part ways for now, I shall see you on the path.
